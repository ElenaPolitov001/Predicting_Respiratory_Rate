Repo for the School of AI Health Hackathon at Accenture NY

# Installation and setup
First clone the repo into your machine. This machine should be big and powerful as the dataset we use will be as much as 1.2GB in size. We used an AWS p3.2xlarge and still had problems with space. 

Second nstall the requirements. You can do this by running `pip install --upgrade -r requirements.txt` in the terminal. 

Third you will want to download and setup the data. Open the notebook titled _download_data.ipynb_ and run all the cells inside it. This will generate some new CSV files inside three different folders. The folders are called _csv_, _txt_ and _person_csvs_. Inside the latter will be _all_people.csv_. This 1.2GB file contains all the data we will use for training and predicting so you if you are running low on space feel free to delete all the other csv files leaving just _all_people.csv_ inside _people_csvs_ once the notebook has finished running.

# Running the model
Run the notebook called _modelling.ipynb_. Inside this notebook you will see a selection of models with just RandomForestRegressor left uncommented. This is because it was definitely our strongest model. If you want to try out others, you can uncomment them but be warned that the KNN model and everything after it is very big (the KNN model is ~850MB). Commented out also is the ability to save these models into a new folder called _saved_models_. Uncomment it if you would like to do this.

At the end of the notebook a table will be generated showing each model's performance. If you are unable or uninclined to train these models, look in the presentation pdf in this repo to view the results.

# Investigating the model
The final notebook is _random_forest_performance.ipynb_. As our best model, we added this notebook to learn more about how well it was fitting the data and which input features were the most import to it. If have saved the model, the code to load it is in there, commented out. If not, you can also train it once again. 

Note that the attributes and methods we use to investigate the model and plot the charts are specific to SciKit Learns RandomForestClasdsifier module. Other modules are like to only have some, or perhaps even none, of these attributes and methods and therefore you will not be able to explore other model architectures in this particular notebook.

# And now follows some discussion of why this dataset is interesting and why our model might be important...
Using a selection of supervised machine learning techniques, we are able to predict any user’s respiratory rate in real time with data readily accessible with a smart device health monitor. Training on solely the BIDMC Dataset consisting of 53 patients, our algorithm is able to achieve R2 of 0.9 and deliver instantaneous results from any modern, portable device. A device which can measure respiratory rate (impedance pneumography device) can cost between $200 - $50,000 and requires professional administration. A respiratory rate measurement is absolutely required for patients who are in critical care, under anaesthesia, or who have pulmonary abnormalities such as pneumonia. In modern hospitals this algorithm will aid in diagnosing potentially critically ill patients by producing results quicker and cheaper; and for those hospitals without modern equipment, it will undoubtedly save lives. 

In our examination of the data files presented in the BIDMC dataset, we chiefly noted that there are three time-series signals presented - those collected from a PPG, an ECG, and an impedance pneumography (IP) device. Of the three, IP devices were significantly more difficult to find as a portable monitoring product while ECG and PPG are commonly seen sold as stand-alone units or integrating with smartphones and smartwatches. Moving forward, we chose to explore how well we could predict impedance pneumography data in real time using the combination of all other features we could obtain from the PPG and ECG sensors.

As explained in the dataset repository, all data recording were taken at rest from patients admitted to the intensive care unit. Importantly, this means that each signal could be inferred to be stationary, and we could derive summary statistical representations for each. Next, after cleaning the data for mis-matched frequencies and concatenating different patient data into a single training database, we were able to feed the labelled database into a number of supervised machine learning algorithms. In our current scope, we’ve selected the best performer, a random forest regressor, to represent our performance. In this experiment, we’ve achieved a normalized MAE of 0.06 and an R2 value of 0.9.

Respiratory rate is a crucial measurement for many patients, it is commonly used to aid in hospice care and diagnosis of pulmonary abnormalities, including pneumonia, hypercarbia, and embolisms. While modern hospitals may readily have access to IP devices for in-patient procedures, reducing administrative complexity and patient prep time is invaluable. Additionally, as many doctors and hospitals encourage digital visits via phone, video, and messaging, having a plethora of at-home measurements - especially respiratory rate - will significantly reduce hospital and patient costs while simultaneously improving patient care. With wearable devices, measurements can be taken throughout the day and night, with doctors being able to revisit patient-reported abnormalities in addition to proactively checking in with patients who have sudden or chronic poor respiratory rate readings. As smart devices continue to penetrate the mobile marketplace, low-cost ECG and PPG sensors will also become available world-wide. Third world nations and impoverished regions without access to a well-equipped hospital will need to have access to respiratory rate as a key data point in their ability to diagnose serious illness - whether at home or at their nearest care facility. Our proposed solution requires minimal computation to deploy, no additional costs over the integrated sensors, and can be pushed to current devices which can already utilize ECG and PPG information. We are confident we can continue to build upon our performance utilizing the state of the art machine learning techniques and hope to deliver our solution globally to those whom it would be vital.
